![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)
# FIAP Solution Sprint Brazilian E-commerce

## √çndice
* [Introdu√ß√£o](#introdu√ß√£o)
* [Descri√ß√£o do Projeto](#descri√ß√£o-do-projeto)
* [Ecossistema Hadoop com Docker](#ecossistema-hadoop-com-docker)
* [Notebooks](#notebooks)
* [Visualiza√ß√£o dos Dados](#visualiza√ß√£o-dos-dados)

## üìåIntrodu√ß√£o
O ecossistema hadoop √© composto por diversas ferramentas, com o objetivo de utilizar os frameworks iremos analisar um dataset disponibilizado no Kaggle.

Um conjunto de dados p√∫blicos de com√©rcio eletr√¥nico brasileiro foi fornecido pela Olist(https://olist.com/) no Kaggle (https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce), este dataset cont√©m dados comerciais reais de 100 mil pedidos de 2016 a 2018 realizados em diversos mercados no Brasil. Os dados foram anonimizados e as refer√™ncias √†s empresas e parceiros no texto da revis√£o foram substitu√≠das pelos nomes das grandes casas de Game of Thrones. As tabelas do dataset se relacionam da seguinte forma:
![image](https://user-images.githubusercontent.com/49615846/165148902-58dcff90-dcaa-4637-85c8-b76a7f880ab0.png)

Algumas perguntas devem ser respondidas sobre este conjunto de dados:
* Segmentar os clientes por geolocaliza√ß√£o.
* Total de pedidos por per√≠odo e categorias.
* Total de pagamentos por m√©todo de pagamento.
* Notas das avalia√ß√µes.
* Vendedores x vendas.
* Produtos mais vendidos.

## üìåDescri√ß√£o do Projeto
Desenhamos a seguinte arquitetura para manipularmos os dados deste dataset e responder as perguntas.

![image](https://user-images.githubusercontent.com/49615846/165752994-d7ed13db-1e58-4c2f-acf3-4cf0be87e293.png)
| Item 	| Ferramenta       	| Descri√ß√£o                                                                                                                                                                                                                                                                                                                                                                                                  	|
|:----:	|------------------	|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|
|   1  	| Fonte - Kaggle   	| O dataset Olist est√° dispon√≠vel para download no site Kaggle.                                                                                                                                                                                                                                                                                                                                              	|
|   2  	| Fonte - Correios 	| Site para consultar endere√ßos por CEP, aceita pesquisa usando apenas o prefixo (5 d√≠gitos).                                                                                                                                                                                                                                                                                                                	|
|   3  	| Fonte - Geopy    	| Biblioteca Python para consultar coordenadas por endere√ßo, inserindo a cidade e endere√ßo, latitude e longitude ser√£o retornados.                                                                                                                                                                                                                                                                           	|
|   4  	| Apache PySpark   	| Nesta etapa ser√° utilizada a API do Kaggle para fazer o download do dataset no formato CSV.                                                                                                                                                                                                                                                                                                                	|
|   5  	| Jupyter Notebook 	| Aplica√ß√£o web onde √© poss√≠vel editar e executar scripts de programa√ß√£o, os scripts PySpark ser√£o criados e executados neste ambiente.                                                                                                                                                                                                                                                                      	|
|   6  	| HDFS             	| O Hadoop Distributed File System √© um sistema de armazenamento distribu√≠do, um namenode (n√≥ mestre) gerencia e armazena metadados dos arquivos, que s√£o armazenados em 1 ou mais datanodes (n√≥s trabalhadores), dando escalabilidade ao armazenamento. Na landing zone os arquivos ser√£o armazenados em sua forma bruta, sem tratamento.                                                                   	|
|   7  	| Apache PySpark   	| Nesta etapa os arquivos CSV do dataset armazenados na landing zone ser√£o transformados para o formato ORC.                                                                                                                                                                                                                                                                                                 	|
|   8  	| Apache PySpark   	| A tabela de geolocaliza√ß√£o possui como chave primaria os primeiros 5 digitos do CEP, e traz qual a cidade, estado e coordenadas, por√©m iremos atualizar as informa√ß√µes de cidade e estado com base nos dados da empresa Correios, esses dados ser√£o consultados com uso da biblioteca python requests no site de busca de CEP do Correios ( https://buscacepinter.correios.com.br/app/endereco/index.php). 	|
|   9  	| Apache PySpark   	| Ap√≥s os dados de CEP serem atualizados com cidade e estado iremos atualizar as coordenadas com o uso da biblioteca python geopy, √© informado o endere√ßo e a biblioteca retorna a latitude e longitude.                                                                                                                                                                                                     	|
|  10  	| HDFS             	| Os dados tratados ser√£o armazenados em outra pasta do HDFS.                                                                                                                                                                                                                                                                                                                                                	|
|  11  	| Apache HUE       	| O Apache HUE √© um editor SQL open-source, ser√° utilizado como uma User Interface (UI) para auxiliar nas consultas SQL no Hive.                                                                                                                                                                                                                                                                             	|
|  12  	| Apache Hive      	| Software de data warehouse que facilita a leitura, escrita e manipula√ß√£o de grandes datasets armazenados em armazenamento distribu√≠do (HDFS) usando SQL.                                                                                                                                                                                                                                                   	|
|  13  	| Metabase         	| Dataviz das tabelas criadas no Hive, respondendo as quest√µes levantadas.                                                                                                                                                                                                                                                                                                                                  	|
## üìåEcossistema Hadoop Com Docker
<br> Esse setup vai criar dockers com os frameworks HDFS, Hive, Presto, Spark, Jupyter, Hue,  Metabase, Mysql.
<br>  

### SOFTWARES NECESS√ÅRIOS
#### Para a cria√ß√£o e uso do ambiente vamos utilizar o git e o Docker 
   * Instala√ß√£o do Docker Desktop no Windows [Docker Desktop](https://hub.docker.com/editions/community/docker-ce-desktop-windows) ou o docker no [Linux](https://docs.docker.com/install/linux/docker-ce/ubuntu/)
   *  [Instala√ß√£o do git](https://git-scm.com/book/pt-br/v2/Come%C3%A7ando-Instalando-o-Git)
   
### SETUP
*OBS: Esse passo deve ser realizado apena uma vez. Ap√≥s o ambiente criado, utilizar o docker-compose para iniciar os containers como mostrado no t√≥pico INICIANDO O AMBIENTE*

#### Cria√ß√£o do diret√≥rio docker:
*OBS: Criar um diret√≥rio chamado docker*

   *  Sugest√£o no Windows:
      *  Criar na raiz do seu drive o diret√≥rio docker
         ex: C:\docker
          
   * Sugest√£o no Linux:
      * Criar o diret√≥rio na home do usu√°rio
        ex: /home/user/docker

#### Em um terminal/DOS, dentro diret√≥rio docker, realizar o clone do projeto no github
          git clone https://github.com/Gabrielvinicius27/desafio_fase1_fiap_engdados

#### No diret√≥rio bigdata_docker vai existir os seguintes objetos
![ls](ls.JPG)
   
### INICIANDO O AMBIENTE
   
  *No Windows abrir PowerShell, do Linux um terminal*

### No terminal, no diretorio bigdata_docker, executar o docker-compose
          docker-compose up -d        

### Verificar imagens e containers
 
         docker image ls

![image](https://user-images.githubusercontent.com/49615846/165780971-03474480-c1c1-46ea-b8b8-c3214b183d35.png)

         docker container ls
         
![image](https://user-images.githubusercontent.com/49615846/165781325-c2f867da-2124-42b4-ad6f-c283db2c0b57.png)


### SOLUCIONANDO PROBLEMAS 
   
  *No Windows abrir o Docker Quickstart Terminal*

#### Parar um containers
         docker stop [nome do container]      

#### Parar todos containers
         docker stop $(docker ps -a -q)
  
#### Remover um container
         docker rm [nome do container]

#### Remover todos containers
         docker rm $(docker ps -a -q)         

#### Dados do containers
         docker container inspect [nome do container]

#### Iniciar um container
         docker-compose up -d [nome do container]

#### Iniciar todos os containers
         docker-compose up -d 

#### Acessar log do container
         docker container logs [nome do container] 

#### Acesso WebUI dos Frameworks
 
* HDFS *http://localhost:50070*
* Presto *http://localhost:8080*
* Metabase *http://localhost:3000*
* Jupyter Spark *http://localhost:8889*
* Hue *http://localhost:8888*
* Spark *http://localhost:4040*

### Acesso por shell

   ##### HDFS

          docker exec -it datanode bash

### Acesso JDBC

   ##### MySQL
          jdbc:mysql://database/employees

   ##### Hive

          jdbc:hive2://hive-server:10000/default

   ##### Presto

          jdbc:presto://presto:8080/hive/default

### Usu√°rios e senhas

   ##### Hue
    Usu√°rio: admin
    Senha: admin

   ##### Metabase
    Usu√°rio: bigdata@class.com
    Senha: bigdata123 

   ##### MySQL
    Usu√°rio: root
    Senha: secret

### Imagens   

[Docker Hub](https://hub.docker.com/u/fjardim)

### Documenta√ß√£o Oficial

* https://prestodb.io/
* https://spark.apache.org/
* https://www.metabase.com/
* https://jupyter.org/
* https://hadoop.apache.org/
* https://hive.apache.org/
* https://gethue.com/
* https://github.com/yahoo/CMAK
* https://www.docker.com/

## üìåNotebooks
### data/notebooks/Desafio1_FIAP/1_Kaggle Dataset Ingestion to HDFS.ipynb
<details>
<summary>clique para ver explica√ß√£o</summary>
Este Notebook faz o download do dataset do Kaggle no formato CSV, utilizando a biblioteca python kaggle basta um token de autentica√ß√£o, que foi gerado no site Kaggle em ‚ÄúYour Profile‚Äù, ‚ÄúAccount‚Äù, em API bot√£o ‚ÄúGenerate New API Token‚Äù, um arquivo json ser√° gerado e deve ser armazenado na pasta informada na vari√°vel de ambiente $KAGGLE_CONFIG_DIR, caso necess√°rio permiss√µes podem ser dadas com chmod

```python
import os
base_path = "/mnt/notebooks/Desafio1_FIAP"
os.environ["KAGGLE_CONFIG_DIR"] = f'{base_path}/kaggle_config_dir/'
!chmod 600 /mnt/notebooks/Desafio1_FIAP/kaggle_config_dir/kaggle.json
```

O m√©todo kaggle.api_dataset_download_files faz o download do dataset no path dos par√¢metros, caso desejar os arquivos descompactados √© necess√°rio usar unzip=True.
```python
import kaggle
kaggle.api.authenticate()

kaggle.api.dataset_download_files('olistbr/brazilian-ecommerce', 
                                  path='/mnt/notebooks/Individual_Desafio1_FIAP/olist_dataset', 
                                  unzip=True)
```

Os arquivos csv do dataset foram armazenados no HDFS com auxilio da biblioteca hdfs para fazer a conex√£o, pandas para formatar o arquivo csv mantendo o cabe√ßalho e removendo o index. Para se conectar ao HDFS foi necess√°rio informar alguns par√¢metros de conex√£o, incluindo o endere√ßo URL do namenode e a porta 50070, tamb√©m foi definida uma estrat√©gia de retry. Os par√¢metros foram definidos conforme c√≥digo abaixo:
```python
import requests
import os
import pandas as pd 
import hdfs
import urllib3

from hdfs import InsecureClient
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util import Retry

max_threads = 50
session = requests.Session()

retry_strategy = Retry(
    total=10,
    connect=10,
    read=10,
    redirect=10,
    status_forcelist=[429, 500, 502, 503, 504],
    method_whitelist=["HEAD", "GET", "OPTIONS"],
)

adapter = HTTPAdapter(
    max_retries=retry_strategy, pool_connections=max_threads, pool_maxsize=max_threads,
)

session.mount("https://", adapter)
session.mount("http://", adapter)

# client usando IP do host docker
client = 'http://192.168.56.1:50070'

# Client HDFS
hdfs_client = InsecureClient(client, session=session)
```

Os arquivos csv foram gravados numa pasta que definimos como landing_zone, que tem como objetivo armazenar os dados em sua forma original, mantendo o formato csv.
```python
# Gravar o arquivo csv no HDFS
for filename in os.listdir(f'{base_path}/olist_dataset'):
    df = pd.read_csv(f'{base_path}/olist_dataset/{filename}', sep =',')
    df.replace(to_replace=[r"\\t|\\n|\\r", "\t|\n|\r"], value=["",""], regex=True, inplace=True)
    try:
        with hdfs_client.write(f'/datalake/landing_zone/{filename}', overwrite = True, encoding='utf-8') as writer:
            df.to_csv(writer, header=True, index=False)
        print(f"{filename} Gravado com sucesso")
    except hdfs.util.HdfsError as e:
        print(f"{filename} falhou")
        print(f"[ERRO] {e}")
    except urllib3.exceptions.NewConnectionError as e:
        print(f"{filename} falhou")
        print(f"[ERRO] {e}")
    except Exception as e:
        print(e)
```
A imagem abaixo mostra os arquivos armazenados no HDFS, esta √© a interface do Apache HUE:
![image](https://user-images.githubusercontent.com/49615846/165817162-337b08dc-0c44-4237-8209-2ab7a6e41007.png)
  
Por fim os arquivos s√£o convertidos para ORC com auxilio da biblioteca pyspark, o formato ORC permite particionamento, compress√£o e schema, funcionalidades que podem ser utilizadas futuramente.

A fun√ß√£o spark.read.csv precisa do par√¢metro inferSchema=True para o spark definir um data type para cada campo, √© importante para que o Metabase funcione corretamente, na cria√ß√£o da tabela os datatypes precisam ser iguais aos do arquivo ORC.
Um dos campos do dataset √© o zip_code_prefix, CEP, ele cont√©m 5 digitos e pode conter zeros a esquerda, por isso este campo precisou ser definido com tipo string em alguns arquivos.
  
√â importante manter o cabe√ßalho com o nome das colunas no arquivo, pois quando a tabela hive for criada com location no caminho desse arquivo o schema da tabela deve bater com o schema do arquivo, as colunas devem possuir o mesmo nome, incluindo a distin√ß√£o dos caracteres min√∫sculos e mai√∫sculos.
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import lpad

spark = SparkSession \
    .builder \
    .appName("Ingest Olist Dataset") \
    .getOrCreate()

landing_zone = '/datalake/landing_zone/'
files = hdfs_client.list(landing_zone)

for filename in files:
    csv = spark.read.csv(f'{landing_zone}/{filename}', header = True, inferSchema=True, sep = ',')
    if filename == 'olist_customers_dataset.csv':
        csv = csv.withColumn('customer_zip_code_prefix', lpad(csv.customer_zip_code_prefix, 5, '0'))
    elif filename == 'olist_sellers_dataset.csv':
        csv = csv.withColumn('seller_zip_code_prefix', lpad(csv.seller_zip_code_prefix, 5, '0'))
    elif filename == 'olist_geolocation_dataset.csv':
        csv = csv.withColumn('geolocation_zip_code_prefix', lpad(csv.geolocation_zip_code_prefix, 5, '0'))
    csv.printSchema()
    orc_name = filename.replace('csv', 'orc')
    csv.write.orc(f'/datalake/dadosbrutos/{orc_name}', 'overwrite')
```
Arquivos armazenados no HDFS
![image](https://user-images.githubusercontent.com/49615846/165946296-bd582fe7-7ee7-4cb8-90bc-47784c1382ca.png)
</details>

### data/notebooks/Desafio1_FIAP/2_Consulta Endere√ßo por CEP com 5 digitos no Site do correio.ipynb
<details>
<summary>clique para ver explica√ß√£o</summary>
Este notebook tem como objetivo consultar todos os CEPs armazenados no arquivo olist_geolocation_dataset.orc para encontrar a cidade e estado, apenas o prefixo dos CEPs √© fornecido, caso o CEP completo estivesse dispon√≠vel seria poss√≠vel utilizar a biblioteca pycep, neste caso ser√° necess√°rio usar o site da empresa Correios que aceita CEPs parciais. 
  
Primeiro criamos um dataframe pyspark para o arquivo olist_geolocation_dataset.orc
```python
from pyspark.sql import SparkSession
from pyspark.sql import HiveContext

hive_context = HiveContext(sc)

spark = SparkSession \
    .builder \
    .appName("API Correios") \
    .enableHiveSupport() \
    .getOrCreate()

geo = spark.read.orc('/datalake/dadosbrutos/olist_geolocation_dataset.orc')
```
Separamos os CEPs distintos, foram encontrados 19015 CEPs distintos neste arquivo.
```python
cep_array = [str(row.geolocation_zip_code_prefix) for row in geo.select('geolocation_zip_code_prefix').distinct().collect()]
```
Como iremos consultar um grande volume de CEPs uma consulta s√≠ncrona levaria muito tempo, pois o programa iria aguardar a resposta do site para seguir para o pr√≥ximo passo, usando consultas ass√≠ncronas a execu√ß√£o √© muito mais r√°pida, pois o programa recebe as respostas do site em momentos futuros e o resto do programa continua executando.

Para atingir esse objetivo usamos a biblioteca asyncio e requests, a URL √© 'https://buscacepinter.correios.com.br/app/endereco/carrega-cep-endereco.php' e consultamos com um m√©todo POST informando qual o CEP e qual o tipo de endere√ßo, no nosso caso retornamos todos os tipos, o site retorna com uma lista de CEPs que correspondem com os 5 digitos informados, n√≥s selecionamos o primeiro resultado encontrado com o campo cep correto, para um prefixo de CEP a cidade e estado ser√° sempre a mesma, o que muda s√£o os logradouros que nesse caso n√£o ser√£o usados.

O c√≥digo a seguir cria as fun√ß√µes get_address, get_all_addresses e consulta_lote. Dado o grande volume de CEPs iremos separar a consulta em lotes de 1000 consultas, caso aconte√ßa alguma falha no meio do programa n√£o iremos perder todo o progresso, a fun√ß√£o consulta_lote inicia o loop ass√≠ncrono para cada lote. A fun√ß√£o get_all_addresses gera as tasks ass√≠ncronas dentro de uma sess√£o, cada task corresponde a consulta de um CEP. A fun√ß√£o get_address faz a consulta no site da empresa Correios e aguarda a resposta para inserir na lista de endere√ßos que ir√° gerar um dataframe do lote.
  
Por fim utilizamos numpy.array_split para criar os lotes de CEPs.
```python

import asyncio
import time
import aiohttp
import nest_asyncio
import pandas as pd
import json
from pyspark.sql import Row

global URL
# URL do site do correios
URL = 'https://buscacepinter.correios.com.br/app/endereco/carrega-cep-endereco.php'
global ceps_com_erro
ceps_com_erro = []
# Fun√ß√£o para pegar o primeiro resultado da pesquisa de CEP com apenas 5 digitos
async def get_address(session, cep):
    async with session.post(url=URL, data={'endereco': cep, 'tipoCEP': 'ALL'}) as response:
        response = await response.text()
        try:
            for i in range(len(json.loads(response)["dados"])):
                data = json.loads(response)["dados"][i]
                if data["cep"] != '' and data["cep"][0:5] == cep: 
                    data_selected = {
                        "cep": data["cep"],
                        "uf": data["uf"],
                        "cidade": data["localidade"]
                    }
                    results.append(data_selected)
                    print(f"{str(len(results)).zfill(6)} CEPs consultados", end="\r")
                    break
        except Exception as e:
            #print(f"ERRO: {e}", end="\r")
            ceps_com_erro.append(cep)
            pass

# Fun√ß√£o para criar as tasks ass√≠ncronas, uma task para cada cep
async def get_all_addresses(ceps):
    async with aiohttp.ClientSession() as session:
        tasks = []
        for cep in ceps:
            task = asyncio.ensure_future(get_address(session, cep))
            tasks.append(task)
        await asyncio.gather(*tasks, return_exceptions=False)
        

# Fun√ß√£o prncipal para iniciar o loop ass√≠ncrono e criar o Dataframe com os resultados
def consulta_lote(ceps_array):
    global results
    results = []
    nest_asyncio.apply()
    start_time = time.time()
    asyncio.get_event_loop().run_until_complete(get_all_addresses(ceps_array))
    
    df = spark.createDataFrame((Row(**x) for x in results))
    
    duration = time.time() - start_time
    print(f"Downloaded {len(ceps_array)} ceps in {duration/60} minutes")
    return df
```
```python
import math
import numpy as np

start_time = time.time()

tamanho_lote = 1000
qtd_lotes = math.floor(len(cep_array)/tamanho_lote)

print(f"Iniciando a consulta de {qtd_lotes} lotes com aprox. {tamanho_lote} ceps cada.")
cep_lotes = np.array_split(cep_array, qtd_lotes)

dataframes = {}
counter = 0
for lote in cep_lotes:
    counter += 1
    print(f"Consultando lote {counter}")
    dataframes[f"df_part{counter}"] = consulta_lote(lote)

duration = time.time() - start_time
print(f"Tempo total da carga: {duration/60} minutos")
print(f"Total de CEPs n√£o encontrados: {len(ceps_com_erro)}")
```
 
Resultado da execu√ß√£o: 19015 CEPs consultados em aproximadamente 3 minutos
![image](https://user-images.githubusercontent.com/49615846/165949926-b7496d4a-c028-4797-83db-4db6af0686db.png)

Ap√≥s a uni√£o dos dataframes dos lotes em um datarame final o resultado √© esse:
![image](https://user-images.githubusercontent.com/49615846/165950104-3ceeaf05-1eb0-43ec-9364-d16a74e997cf.png)

O resultado foi gravado no HDFS.
</details>

### data/notebooks/Desafio1_FIAP/3_Consulta Coordenadas por Endere√ßo.ipynb
<details>
<summary>clique para ver explica√ß√£o</summary>
Com as cidades e estados atualizados √© preciso atualizar as coordenadas, Geopy √© uma biblioteca python para dados de geolocaliza√ß√£o, ao informar uma cidade e estado as coordenadas s√£o retornadas.

O c√≥digo abaixo envia de forma s√≠ncrona cada cidade e estado distintos para o Geopy que retorna as latitudes e longitudes, em aproximadamente 3,5 horas mais de 18 mil endere√ßos s√£o processados.
  
```python
from geopy.geocoders import Nominatim
import time

start_time = time.time()
# Cidade e estado distintos
cidades_ufs = geo.select('cep_5_digitos','cidade','uf').distinct().collect()
qtde_cidades_ufs = geo.select('cep_5_digitos','cidade','uf').distinct().count()

coords = []
counter = 0

# Consulta com o Geopy qual as coordenadas para cada conjunto cidade, estado.
for linha in cidades_ufs:
    print(f"{counter}¬∫ Consulta de {qtde_cidades_ufs}", end="\r")
    try:
        geolocator = Nominatim(user_agent="test_app", timeout=15)
        location = geolocator.geocode(f'{linha["cidade"]}, {linha["uf"]}')
        coords.append([linha['cep_5_digitos'], location.raw["lat"], location.raw["lon"]])
        counter += 1
    except Exception as e:
        print(e)
        pass

# Cria o dataframe com as coordenadas de cada cep
coords_df = spark.createDataFrame(coords, schema=["cep_5_digitos","lat", "lon"])
coords_df.show(truncate=False)
duration = time.time() - start_time
print(f"Tempo total: {duration/60} minutes")
```
Resultado:
  
  ![image](https://user-images.githubusercontent.com/49615846/166977830-c0b5229d-a29f-41c1-8015-0a12af6672a8.png)

√â feito um join entre a tabela com cidade e estados atualizados e esta tabela de coordenadas.
  
  ![image](https://user-images.githubusercontent.com/49615846/166978010-79d82704-c905-4840-949e-baba50dc99bd.png)
</details>

### data/notebooks/Desafio1_FIAP/4_Criar Tabelas no HIVE.ipynb
<details>
<summary>clique para ver explica√ß√£o</summary>
Os resultados das etapas anteriores foram gravados em arquivos ORC, com o Hive conseguimos criar tabelas externas com location nos arquivos criados, nessas tabelas definimos as colunas e data types.

Para esta tarefa foi utilizada a biblioteca Jaydebeapi, que faz a conex√£o JDBC com o Hive, e foi utilizada a biblioteca PyGithub para fazer o download do driver JDBC hive. No reposit√≥rio timveil/hive-jdbc-uber-jar est√£o armazenadas as releases do drive, o c√≥digo abaixo faz o download dos assets da √∫ltima release dispon√≠vel.
```python
from github import Github
import requests
import os

base_path = "/mnt/notebooks/Desafio1_FIAP"

g = Github()
asset = g.get_repo('timveil/hive-jdbc-uber-jar').get_latest_release().get_assets()[0]
url = asset.browser_download_url
print(asset.name)
print(url)

response = requests.get(url)
open(f'{base_path}/driver/{asset.name}', 'wb').write(response.content)
response.close()
``` 
Ap√≥s o download do driver √© poss√≠vel estabelecer a conex√£o, √© preciso indicar qual o endere√ßo de onde o driver est√° armazenado, o driver que ser√° usado, o database e url de conex√£o JDBC
```python
import jaydebeapi

# Jar
base_path = "/mnt/notebooks/Desafio1_FIAP"
hivejar = f"{base_path}/driver/{asset.name}"
driver = "org.apache.hive.jdbc.HiveDriver"
database = "db_olist"
# JDBC connection string
url=("jdbc:hive2://hive-server:10000/db_olist")
dadosbrutos_folder = '/datalake/dadosbrutos'

# Connect to HiveServer2 
conn = jaydebeapi.connect(jclassname=driver, url=url, jars=hivejar)
cursor = conn.cursor()
```
Para criar as tabelas foi utilizado um dicion√°rio python com os scripts DDL que ser√£o executados, cada chave indica o nome da tabela e o valor indica o script, as tabelas ser√£o armazenadas no formato ORC e sua location √© o endere√ßo do arquivo ORC que foi gravado no HDFS. Cada nome e data type de coluna deve ser igual ao nome e data type da coluna do arquivo, foi utilizado o m√©todo printSchema() do DataFrame Spark para verificar o nome e data type das colunas no ORC.
```python
# Tabelas que ser√£o criadas
tabelas = {'geolocation': f"""
            CREATE EXTERNAL TABLE {database}.geolocation
            (
                geolocation_zip_code_prefix STRING COMMENT '5 primeiros digitos do CEP.',
                geolocation_lat DOUBLE COMMENT 'Latitude.',
                geolocation_lng DOUBLE COMMENT 'Longitude.',
                geolocation_city STRING COMMENT 'Cidade.',
                geolocation_state STRING COMMENT 'Estado.'
            ) COMMENT 'Tabela com CEPs brasileiros, e dados de geolocaliza√ß√£o.'
            STORED AS ORC
            LOCATION '/datalake/dadosbrutos/olist_geolocation_dataset.orc/'
            """,
           
           'customers': f"""
            CREATE EXTERNAL TABLE {database}.customers
            (
                customer_id STRING COMMENT 'Chave para a tabela de ordens. Cada ordem tem um customer_id √∫nico.',
                customer_unique_id STRING COMMENT 'Identificador √∫nico do cliente.',
                customer_zip_code_prefix STRING COMMENT 'Primeiros 5 digitos do CEP do cliente.',
                customer_city STRING COMMENT 'Cidade do endere√ßo do cliente.',
                customer_state STRING COMMENT 'Estado do endere√ßo do cliente.'
            ) COMMENT 'Tabela com clientes e seus dados de geolocaliza√ß√£o.'
            STORED AS ORC
            LOCATION '/datalake/dadosbrutos/olist_customers_dataset.orc/'
            """,
           
           'order_items': f"""
            CREATE EXTERNAL TABLE {database}.order_items
            (
                order_id STRING COMMENT 'Identificador √∫nico da ordem.',
                order_item_id INT COMMENT 'N√∫mero sequencial identificando o n√∫mero de itens inclu√≠dos na mesma ordem.',
                product_id STRING COMMENT 'Identificador √∫nico do produto.',
                seller_id STRING COMMENT 'Identificador √∫nico do vendedor.',
                shipping_limit_date TIMESTAMP COMMENT 'Mostra a data limite de entrega do vendedor para o parceiro log√≠stico.',
                price DOUBLE COMMENT 'Pre√ßo do item.',
                freight_value DOUBLE COMMENT 'Valor do frete do item (se um pedido tiver mais de um item o valor do frete √© dividido entre os itens).'
            ) COMMENT 'Tabela com dados dos itens comprados em uma ordem.'
            STORED AS ORC
            LOCATION '/datalake/dadosbrutos/olist_order_items_dataset.orc/'
            """,
           
           'order_payments': f"""
            CREATE EXTERNAL TABLE {database}.order_payments
            (
                order_id STRING COMMENT 'Identificador √∫nico da ordem.',
                payment_sequential INT COMMENT 'Um cliente pode pagar uma ordem com mais de um m√©todo de pagamento, a coluna indica a sequencia',
                payment_type STRING COMMENT 'M√©todo de pagamento escolhido.',
                payment_installments INT COMMENT 'N√∫mero de parcelas escolhidas pelo cliente.',
                payment_value DOUBLE COMMENT 'Valor do pagamento.'
            ) COMMENT 'Tabela com dados de pagamento da ordem.'
            STORED AS ORC
            LOCATION '/datalake/dadosbrutos/olist_order_payments_dataset.orc/'
            """,
           
           'order_reviews': f"""
            CREATE EXTERNAL TABLE {database}.order_reviews
            (
                review_id STRING COMMENT 'Identificador √∫nico do review.',
                order_id STRING COMMENT 'Identificador √∫nico da ordem.',
                review_score INTEGER COMMENT 'Nota de 1 a 5 dada pelo cliente na pesquisa de satisfa√ß√£o',
                review_comment_title STRING COMMENT 'T√≠tulo do coment√°rio.',
                review_comment_message STRING COMMENT 'Mensagem do coment√°rio.',
                review_creation_date TIMESTAMP COMMENT 'Data em que a pesquisa de satisfa√ß√£o foi envada.',
                review_answer_timestamp TIMESTAMP COMMENT 'Data em que a pesquisa de satisfa√ß√£o foi respondida.'
            ) COMMENT 'Tabela com dados dos reviews feitos pelos clientes.'
            STORED AS ORC
            LOCATION '/datalake/dadosbrutos/olist_order_reviews_dataset.orc/'
            """,
           
           'orders': f"""
            CREATE EXTERNAL TABLE {database}.orders
            (
                order_id STRING COMMENT 'Identificador √∫nico da ordem.',
                customer_id STRING COMMENT 'Identificador √∫nico do cliente',
                order_status STRING COMMENT 'Status da ordem (delivered, shipped, etc).',
                order_purchase_timestamp TIMESTAMP COMMENT 'Data da compra.',
                order_approved_at TIMESTAMP COMMENT 'Data de aprova√ß√£o do pagamento.',
                order_delivered_carrier_date TIMESTAMP COMMENT 'Data em que o produto foi entregue ao parceiro log√≠stico.',
                order_delivered_customer_date TIMESTAMP COMMENT 'Data em que o produto foi entregue ao cliente.',
                order_estimated_delivery_date TIMESTAMP COMMENT 'Data de entrega estimada informada ao cliente no momento da compra.'
            ) COMMENT 'Tabela com dados das ordens.'
            STORED AS ORC
            LOCATION '/datalake/dadosbrutos/olist_orders_dataset.orc/'
            """,
           
           'products': f"""
            CREATE EXTERNAL TABLE {database}.products
            (
                product_id STRING COMMENT 'Identificador √∫nico do produto.',
                product_category_name STRING COMMENT 'Nome da categoria do produto.',
                product_name_lenght DOUBLE COMMENT 'N√∫mero de caracteres extra√≠dos do nome do produto.',
                product_description_lenght DOUBLE COMMENT 'N√∫mero de caracteres extra√≠dos da descri√ß√£o do produto.',
                product_photos_qty DOUBLE COMMENT 'N√∫mero de fotos publicadas do produto.',
                product_weight_g DOUBLE COMMENT 'Peso do produto medido em gramas.',
                product_length_cm DOUBLE COMMENT 'Comprimento do produto medido em cent√≠metros.',
                product_height_cm DOUBLE COMMENT 'Altura do produto medida em cent√≠metros.',
                product_width_cm DOUBLE COMMENT 'Largura do produto medida em cent√≠metros.'
            ) COMMENT 'Tabela com dados dos produtos.'
            STORED AS ORC
            LOCATION '/datalake/dadosbrutos/olist_products_dataset.orc/'
            """,
           
           'sellers': f"""
            CREATE EXTERNAL TABLE {database}.sellers
            (
                seller_id STRING COMMENT 'Identificador √∫nico do vendedor.',
                seller_zip_code_prefix STRING COMMENT '5 primeiros digitos do CEP do vendedor.',
                seller_city STRING COMMENT 'Cidade do endere√ßo do vendedor.',
                seller_state STRING COMMENT 'Estado do endere√ßo do vendedor.'
            ) COMMENT 'Tabela com dados dos vendedores.'
            STORED AS ORC
            LOCATION '/datalake/dadosbrutos/olist_sellers_dataset.orc/'
            """,
           
           'product_category_name_translation': f"""
            CREATE EXTERNAL TABLE {database}.product_category_name_translation
            (
                product_category_name STRING COMMENT 'Nome da categoria do produto em portugu√™s .',
                product_category_name_english STRING COMMENT 'Nome da categoria do produto em ingl√™s .'
            ) COMMENT 'Tabela de tradu√ß√£o do nome da categoria do produto.'
            STORED AS ORC
            LOCATION '/datalake/dadosbrutos/product_category_name_translation.orc/'
            """,
           
           'geolocation_correios': f"""
            CREATE EXTERNAL TABLE {database}.geolocation_correios
            (
                cep_5_digitos STRING COMMENT 'Prefixo do CEP.',
                cidade STRING COMMENT 'Cidade.',
                uf STRING COMMENT 'Estado.',
                lat DOUBLE COMMENT 'Latitude.',
                lon DOUBLE COMMENT 'Longitude.'
            ) COMMENT 'Tabela de geolocaliza√ß√£o atualizada com dados dos correios e da biblioteca geopy.'
            STORED AS ORC
            LOCATION '/datalake/dadosbrutos/geolocation_correios_coords.orc'
            """,
           
          }


for item in tabelas:
    # Dropar tabela caso ela j√° existir
    sql = f"""
        DROP TABLE IF EXISTS {database}.{item}
    """
    cursor.execute(sql)
    # Criar a tabela
    query = tabelas[item].strip('\n').strip('\t')
    sql = query
    cursor.execute(sql)
```
Exemplo de visualiza√ß√£o das tabelas no HUE:
  
![image](https://user-images.githubusercontent.com/49615846/166987235-4e5c7d1b-9d87-454c-9451-1dd1f4ffed5c.png)

</details>

## üìåVisualiza√ß√£o dos Dados
Neste projeto foram levantadas as perguntas abaixo, utilizamos o Metabase para responder as quest√µes, com essa ferramenta conseguimos criar visualiza√ß√µes, gr√°ficos e consultas SQL com o uso do Presto que √© um mecanismo de consulta distribu√≠do.

* Segmentar os clientes por geolocaliza√ß√£o.
* Total de pedidos por per√≠odo e categorias.
* Total de pagamentos por m√©todo de pagamento.
* Notas das avalia√ß√µes.
* Vendedores x vendas.
* Produtos mais vendidos.

Resultados obtidos:
### Quantidade de Clientes por Estado
![Alt Text](https://github.com/Gabrielvinicius27/desafio_fase1_fiap_engdados/blob/master/quantidade_clientes_por_estado.gif)

### Quantidade de Clientes por Cidade em S√£o Paulo.
![Alt Text](https://github.com/Gabrielvinicius27/desafio_fase1_fiap_engdados/blob/master/quantidade_clientes_por_cidade_SP.gif)

### Localiza√ß√£o dos Vendedores 
![image](https://user-images.githubusercontent.com/49615846/167004859-5401566a-f1f0-4f7a-adb9-dbee6722c7b3.png)

### Total de Pedidos por Per√≠odo e Categoria
![Alt Text](https://github.com/Gabrielvinicius27/desafio_fase1_fiap_engdados/blob/master/total_pedidos_por_periodo_e_categoria.gif)

### Total de Pagamentos por M√©todo de Pagamento
![Alt_Text](https://github.com/Gabrielvinicius27/desafio_fase1_fiap_engdados/blob/master/total_pagamentos_por_metodo_pagamento.gif)

### M√©dia das avalia√ß√µes por Vendedor
![image](https://user-images.githubusercontent.com/49615846/167005140-51dac559-459e-429c-a7d1-b0055da4d8b7.png)

### Vendedores X Vendas
![image](https://user-images.githubusercontent.com/49615846/167005317-bd49c4b2-b073-4ea3-ad5a-6e68d3d8f16e.png)

### Categorias de Produtos Mais Vendidos
![image](https://user-images.githubusercontent.com/49615846/167005480-bad48f00-d579-4633-8c29-c81f8e25f8ec.png)



###
