{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nest_asyncio\n",
      "  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n",
      "Installing collected packages: nest-asyncio\n",
      "Successfully installed nest-asyncio-1.5.5\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 21.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (270 kB)\n",
      "\u001b[K     |████████████████████████████████| 270 kB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting typing-extensions>=3.7.4; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.6/site-packages (from aiohttp) (18.1.0)\n",
      "Collecting asynctest==0.13.0; python_version < \"3.8\"\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (191 kB)\n",
      "\u001b[K     |████████████████████████████████| 191 kB 19.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna-ssl>=1.0; python_version < \"3.7\"\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/lib/python3.6/site-packages (from yarl<2.0,>=1.0->aiohttp) (2.6)\n",
      "Building wheels for collected packages: idna-ssl\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3161 sha256=0c3770792c9b6c2e2a7e92a77a7a6d257cb1420172ffb0a8e744f00e3e996567\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "Successfully built idna-ssl\n",
      "Installing collected packages: multidict, typing-extensions, yarl, async-timeout, asynctest, frozenlist, aiosignal, charset-normalizer, idna-ssl, aiohttp\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 charset-normalizer-2.0.12 frozenlist-1.2.0 idna-ssl-1.1.0 multidict-5.2.0 typing-extensions-4.1.1 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nest_asyncio\n",
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo a tabela geolocation do HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "hive_context = HiveContext(sc)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Teste API\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "geo = spark.read.orc('/datalake/dadosbrutos/olist_geolocation_dataset.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19015"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo.select('geolocation_zip_code_prefix').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os CEPs distintos em uma lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19015 CEPs distintos encontrados no olist_geolocation_dataset.orc\n"
     ]
    }
   ],
   "source": [
    "cep_array = [str(row.geolocation_zip_code_prefix) for row in geo.select('geolocation_zip_code_prefix').distinct().collect()]\n",
    "print(f\"{len(cep_array)} CEPs distintos encontrados no olist_geolocation_dataset.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultando CEPs no site do Correios com apenas os 5 primeiros digitos\n",
    "* Usando a API pycep conseguimos consultar apenas com o CEP completo, 8 digitos\n",
    "* Consultando direto no site do Correios conseguimos consultar com 5 digitos e descobrir a cidade e estado do CEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import json\n",
    "from pyspark.sql import Row\n",
    "\n",
    "global URL\n",
    "# URL do site do correios\n",
    "URL = 'https://buscacepinter.correios.com.br/app/endereco/carrega-cep-endereco.php'\n",
    "global ceps_com_erro\n",
    "ceps_com_erro = []\n",
    "# Função para pegar o primeiro resultado da pesquisa de CEP com apenas 5 digitos\n",
    "async def get_address(session, cep):\n",
    "    async with session.post(url=URL, data={'endereco': cep, 'tipoCEP': 'ALL'}) as response:\n",
    "        response = await response.text()\n",
    "        try:\n",
    "            for i in range(len(json.loads(response)[\"dados\"])):\n",
    "                data = json.loads(response)[\"dados\"][i]\n",
    "                if data[\"cep\"] != '' and data[\"cep\"][0:5] == cep: \n",
    "                    data_selected = {\n",
    "                        \"cep\": data[\"cep\"],\n",
    "                        \"uf\": data[\"uf\"],\n",
    "                        \"cidade\": data[\"localidade\"]\n",
    "                    }\n",
    "                    results.append(data_selected)\n",
    "                    print(f\"{str(len(results)).zfill(6)} CEPs consultados\", end=\"\\r\")\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            #print(f\"ERRO: {e}\", end=\"\\r\")\n",
    "            ceps_com_erro.append(cep)\n",
    "            pass\n",
    "\n",
    "# Função para criar as tasks assíncronas, uma task para cada cep\n",
    "async def get_all_addresses(ceps):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for cep in ceps:\n",
    "            task = asyncio.ensure_future(get_address(session, cep))\n",
    "            tasks.append(task)\n",
    "        await asyncio.gather(*tasks, return_exceptions=False)\n",
    "        \n",
    "\n",
    "# Função prncipal para iniciar o loop assíncrono e criar o Dataframe com os resultados\n",
    "def consulta_lote(ceps_array):\n",
    "    global results\n",
    "    results = []\n",
    "    nest_asyncio.apply()\n",
    "    start_time = time.time()\n",
    "    asyncio.get_event_loop().run_until_complete(get_all_addresses(ceps_array))\n",
    "    \n",
    "    df = spark.createDataFrame((Row(**x) for x in results))\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Downloaded {len(ceps_array)} ceps in {duration/60} minutes\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a consulta de 19 lotes com aprox. 1000 ceps cada.\n",
      "Consultando lote 1\n",
      "Downloaded 1001 ceps in 0.20070825417836508 minutes\n",
      "Consultando lote 2\n",
      "Downloaded 1001 ceps in 0.19917014042536418 minutes\n",
      "Consultando lote 3\n",
      "Downloaded 1001 ceps in 0.20558483997980753 minutes\n",
      "Consultando lote 4\n",
      "Downloaded 1001 ceps in 0.18404589891433715 minutes\n",
      "Consultando lote 5\n",
      "Downloaded 1001 ceps in 0.18405269384384154 minutes\n",
      "Consultando lote 6\n",
      "Downloaded 1001 ceps in 0.1641371488571167 minutes\n",
      "Consultando lote 7\n",
      "Downloaded 1001 ceps in 0.1727285663286845 minutes\n",
      "Consultando lote 8\n",
      "Downloaded 1001 ceps in 0.1647661288579305 minutes\n",
      "Consultando lote 9\n",
      "Downloaded 1001 ceps in 0.16578193108240763 minutes\n",
      "Consultando lote 10\n",
      "Downloaded 1001 ceps in 0.208054780960083 minutes\n",
      "Consultando lote 11\n",
      "Downloaded 1001 ceps in 0.156492547194163 minutes\n",
      "Consultando lote 12\n",
      "Downloaded 1001 ceps in 0.17083444197972616 minutes\n",
      "Consultando lote 13\n",
      "Downloaded 1001 ceps in 0.1676690419514974 minutes\n",
      "Consultando lote 14\n",
      "Downloaded 1001 ceps in 0.16870800654093424 minutes\n",
      "Consultando lote 15\n",
      "Downloaded 1001 ceps in 0.16477296749750772 minutes\n",
      "Consultando lote 16\n",
      "Downloaded 1000 ceps in 0.16263197660446166 minutes\n",
      "Consultando lote 17\n",
      "Downloaded 1000 ceps in 0.1644183317820231 minutes\n",
      "Consultando lote 18\n",
      "Downloaded 1000 ceps in 0.16328635613123577 minutes\n",
      "Consultando lote 19\n",
      "Downloaded 1000 ceps in 0.20131820837656658 minutes\n",
      "Tempo total da carga: 3.3695267915725706 minutos\n",
      "Total de CEPs não encontrados: 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "tamanho_lote = 1000\n",
    "qtd_lotes = math.floor(len(cep_array)/tamanho_lote)\n",
    "\n",
    "print(f\"Iniciando a consulta de {qtd_lotes} lotes com aprox. {tamanho_lote} ceps cada.\")\n",
    "cep_lotes = np.array_split(cep_array, qtd_lotes)\n",
    "\n",
    "dataframes = {}\n",
    "counter = 0\n",
    "for lote in cep_lotes:\n",
    "    counter += 1\n",
    "    print(f\"Consultando lote {counter}\")\n",
    "    dataframes[f\"df_part{counter}\"] = consulta_lote(lote)\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"Tempo total da carga: {duration/60} minutos\")\n",
    "print(f\"Total de CEPs não encontrados: {len(ceps_com_erro)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unindo os lotes em apenas um dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unindo o df_part2 ao df principal\r",
      "Unindo o df_part3 ao df principal\r",
      "Unindo o df_part4 ao df principal\r",
      "Unindo o df_part5 ao df principal\r",
      "Unindo o df_part6 ao df principal\r",
      "Unindo o df_part7 ao df principal\r",
      "Unindo o df_part8 ao df principal\r",
      "Unindo o df_part9 ao df principal\r",
      "Unindo o df_part10 ao df principal\r",
      "Unindo o df_part11 ao df principal\r",
      "Unindo o df_part12 ao df principal\r",
      "Unindo o df_part13 ao df principal\r",
      "Unindo o df_part14 ao df principal\r",
      "Unindo o df_part15 ao df principal\r",
      "Unindo o df_part16 ao df principal\r",
      "Unindo o df_part17 ao df principal\r",
      "Unindo o df_part18 ao df principal\r",
      "Unindo o df_part19 ao df principal\r"
     ]
    }
   ],
   "source": [
    "df_final = dataframes[\"df_part1\"]\n",
    "#dataframes.pop(\"df_part1\")\n",
    "for df in dataframes:\n",
    "    if df == \"df_part1\": continue\n",
    "    print(f\"Unindo o {df} ao df principal\", end = \"\\r\")\n",
    "    df_final = df_final.union(dataframes[df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18617"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+---+-------------+\n",
      "|cep     |cidade               |uf |cep_5_digitos|\n",
      "+--------+---------------------+---+-------------+\n",
      "|49630970|Siriri               |SE |49630        |\n",
      "|55445000|Batateira            |PE |55445        |\n",
      "|77303970|Rio da Conceição     |TO |77303        |\n",
      "|83450970|Bocaiúva do Sul      |PR |83450        |\n",
      "|48370970|Esplanada            |BA |48370        |\n",
      "|89669970|Ipira                |SC |89669        |\n",
      "|76976970|Primavera de Rondônia|RO |76976        |\n",
      "|45300970|Amargosa             |BA |45300        |\n",
      "|89555970|Ipoméia              |SC |89555        |\n",
      "|86900970|Jandaia do Sul       |PR |86900        |\n",
      "|70078900|Brasília             |DF |70078        |\n",
      "|74605125|Goiânia              |GO |74605        |\n",
      "|59318959|Serra Negra do Norte |RN |59318        |\n",
      "|65927970|Davinópolis          |MA |65927        |\n",
      "|18130649|São Roque            |SP |18130        |\n",
      "|64793970|Coronel José Dias    |PI |64793        |\n",
      "|16250970|Clementina           |SP |16250        |\n",
      "|49290970|Itabaianinha         |SE |49290        |\n",
      "|39590970|Juramento            |MG |39590        |\n",
      "|58819970|Marizópolis          |PB |58819        |\n",
      "+--------+---------------------+---+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df_final = df_final.withColumn('cep_5_digitos', F.col(\"cep\").substr(1,5))\n",
    "df_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escrevendo o resultado final no HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.write.orc('/datalake/dadosbrutos/geolocation_correios.orc', 'overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
