{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /opt/anaconda3/lib/python3.6/site-packages (1.5.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo a tabela geolocation do HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "hive_context = HiveContext(sc)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Teste API\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "geo = spark.read.orc('/datalake/dadosbrutos/olist_geolocation_dataset.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19015"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo.select('geolocation_zip_code_prefix').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os CEPs distintos em uma lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19015 CEPs distintos encontrados no olist_geolocation_dataset.orc\n"
     ]
    }
   ],
   "source": [
    "cep_array = [str(row.geolocation_zip_code_prefix) for row in geo.select('geolocation_zip_code_prefix').distinct().collect()]\n",
    "print(f\"{len(cep_array)} CEPs distintos encontrados no olist_geolocation_dataset.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultando CEPs no site do Correios com apenas os 5 primeiros digitos\n",
    "* Usando a API pycep conseguimos consultar apenas com o CEP completo, 8 digitos\n",
    "* Consultando direto no site do Correios conseguimos consultar com 5 digitos e descobrir a cidade e estado do CEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import json\n",
    "from pyspark.sql import Row\n",
    "\n",
    "global URL\n",
    "# URL do site do correios\n",
    "URL = 'https://buscacepinter.correios.com.br/app/endereco/carrega-cep-endereco.php'\n",
    "global ceps_com_erro\n",
    "ceps_com_erro = []\n",
    "# Função para pegar o primeiro resultado da pesquisa de CEP com apenas 5 digitos\n",
    "async def get_address(session, cep):\n",
    "    async with session.post(url=URL, data={'endereco': cep, 'tipoCEP': 'ALL'}) as response:\n",
    "        response = await response.text()\n",
    "        try:\n",
    "            data = json.loads(response)[\"dados\"][0]\n",
    "            if data[\"cep\"] != '': \n",
    "                data_selected = {\n",
    "                    \"cep\": data[\"cep\"],\n",
    "                    \"uf\": data[\"uf\"],\n",
    "                    \"cidade\": data[\"localidade\"]\n",
    "                }\n",
    "                results.append(data_selected)\n",
    "                print(f\"{str(len(results)).zfill(6)} CEPs consultados\", end=\"\\r\")\n",
    "        except Exception as e:\n",
    "            #print(f\"ERRO: {e}\", end=\"\\r\")\n",
    "            ceps_com_erro.append(cep)\n",
    "            pass\n",
    "\n",
    "# Função para criar as tasks assíncronas, uma task para cada cep\n",
    "async def get_all_addresses(ceps):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for cep in ceps:\n",
    "            task = asyncio.ensure_future(get_address(session, cep))\n",
    "            tasks.append(task)\n",
    "        await asyncio.gather(*tasks, return_exceptions=False)\n",
    "        \n",
    "\n",
    "# Função prncipal para iniciar o loop assíncrono e criar o Dataframe com os resultados\n",
    "def consulta_lote(ceps_array):\n",
    "    global results\n",
    "    results = []\n",
    "    nest_asyncio.apply()\n",
    "    start_time = time.time()\n",
    "    asyncio.get_event_loop().run_until_complete(get_all_addresses(ceps_array))\n",
    "    \n",
    "    df = spark.createDataFrame((Row(**x) for x in results))\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Downloaded {len(ceps_array)} ceps in {duration/60} minutes\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a consulta de 19 lotes com aprox. 1000 ceps cada.\n",
      "Consultando lote 1\n",
      "Downloaded 1001 ceps in 0.13712219397226968 minutes\n",
      "Consultando lote 2\n",
      "Downloaded 1001 ceps in 0.14149274428685507 minutes\n",
      "Consultando lote 3\n",
      "Downloaded 1001 ceps in 0.15122833251953124 minutes\n",
      "Consultando lote 4\n",
      "Downloaded 1001 ceps in 0.13631155093510947 minutes\n",
      "Consultando lote 5\n",
      "Downloaded 1001 ceps in 0.1362757166226705 minutes\n",
      "Consultando lote 6\n",
      "Downloaded 1001 ceps in 0.13811051845550537 minutes\n",
      "Consultando lote 7\n",
      "Downloaded 1001 ceps in 0.1695775548617045 minutes\n",
      "Consultando lote 8\n",
      "Downloaded 1001 ceps in 0.169659690062205 minutes\n",
      "Consultando lote 9\n",
      "Downloaded 1001 ceps in 0.140899391969045 minutes\n",
      "Consultando lote 10\n",
      "Downloaded 1001 ceps in 0.1382043917973836 minutes\n",
      "Consultando lote 11\n",
      "Downloaded 1001 ceps in 0.15547292629877726 minutes\n",
      "Consultando lote 12\n",
      "Downloaded 1001 ceps in 0.1526509722073873 minutes\n",
      "Consultando lote 13\n",
      "Downloaded 1001 ceps in 0.12330770095189413 minutes\n",
      "Consultando lote 14\n",
      "Downloaded 1001 ceps in 0.14208393494288127 minutes\n",
      "Consultando lote 15\n",
      "Downloaded 1001 ceps in 0.1300908366839091 minutes\n",
      "Consultando lote 16\n",
      "Downloaded 1000 ceps in 0.13479898770650228 minutes\n",
      "Consultando lote 17\n",
      "Downloaded 1000 ceps in 0.1456474264462789 minutes\n",
      "Consultando lote 18\n",
      "Downloaded 1000 ceps in 0.1271919886271159 minutes\n",
      "Consultando lote 19\n",
      "Downloaded 1000 ceps in 0.12149235407511393 minutes\n",
      "Tempo total da carga: 2.6923250198364257 minutos\n",
      "Total de CEPs não encontrados: 232\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "tamanho_lote = 1000\n",
    "qtd_lotes = math.floor(len(cep_array)/tamanho_lote)\n",
    "\n",
    "print(f\"Iniciando a consulta de {qtd_lotes} lotes com aprox. {tamanho_lote} ceps cada.\")\n",
    "cep_lotes = np.array_split(cep_array, qtd_lotes)\n",
    "\n",
    "dataframes = {}\n",
    "counter = 0\n",
    "for lote in cep_lotes:\n",
    "    counter += 1\n",
    "    print(f\"Consultando lote {counter}\")\n",
    "    dataframes[f\"df_part{counter}\"] = consulta_lote(lote)\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"Tempo total da carga: {duration/60} minutos\")\n",
    "print(f\"Total de CEPs não encontrados: {len(ceps_com_erro)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unindo os lotes em apenas um dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unindo o df_part2 ao df principal\r",
      "Unindo o df_part3 ao df principal\r",
      "Unindo o df_part4 ao df principal\r",
      "Unindo o df_part5 ao df principal\r",
      "Unindo o df_part6 ao df principal\r",
      "Unindo o df_part7 ao df principal\r",
      "Unindo o df_part8 ao df principal\r",
      "Unindo o df_part9 ao df principal\r",
      "Unindo o df_part10 ao df principal\r",
      "Unindo o df_part11 ao df principal\r",
      "Unindo o df_part12 ao df principal\r",
      "Unindo o df_part13 ao df principal\r",
      "Unindo o df_part14 ao df principal\r",
      "Unindo o df_part15 ao df principal\r",
      "Unindo o df_part16 ao df principal\r",
      "Unindo o df_part17 ao df principal\r",
      "Unindo o df_part18 ao df principal\r",
      "Unindo o df_part19 ao df principal\r"
     ]
    }
   ],
   "source": [
    "df_final = dataframes[\"df_part1\"]\n",
    "#dataframes.pop(\"df_part1\")\n",
    "for df in dataframes:\n",
    "    if df == \"df_part1\": continue\n",
    "    print(f\"Unindo o {df} ao df principal\", end = \"\\r\")\n",
    "    df_final = df_final.union(dataframes[df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+---+-------------+\n",
      "|cep     |cidade               |uf |cep_5_digitos|\n",
      "+--------+---------------------+---+-------------+\n",
      "|76976970|Primavera de Rondônia|RO |76976        |\n",
      "|77303970|Rio da Conceição     |TO |77303        |\n",
      "|45300970|Amargosa             |BA |45300        |\n",
      "|55445000|Batateira            |PE |55445        |\n",
      "|45157970|Cândido Sales        |BA |45157        |\n",
      "|47960970|Angical              |BA |47960        |\n",
      "|49290970|Itabaianinha         |SE |49290        |\n",
      "|48370970|Esplanada            |BA |48370        |\n",
      "|79785971|Angélica             |MS |79785        |\n",
      "|58819970|Marizópolis          |PB |58819        |\n",
      "|16250970|Clementina           |SP |16250        |\n",
      "|89555970|Ipoméia              |SC |89555        |\n",
      "|45540991|Gongogi              |BA |45540        |\n",
      "|77930970|Axixá do Tocantins   |TO |77930        |\n",
      "|83450970|Bocaiúva do Sul      |PR |83450        |\n",
      "|78888972|Nova Ubiratã         |MT |78888        |\n",
      "|89669970|Ipira                |SC |89669        |\n",
      "|39590970|Juramento            |MG |39590        |\n",
      "|69460970|Coari                |AM |69460        |\n",
      "|70078900|Brasília             |DF |70078        |\n",
      "+--------+---------------------+---+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df_final = df_final.withColumn('cep_5_digitos', F.col(\"cep\").substr(1,5))\n",
    "df_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escrevendo o resultado final no HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.write.orc(f'/datalake/dadosbrutos/geolocation_correios.orc', 'overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
